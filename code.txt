# ==================== Ù†ØµØ¨ Ùˆ Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… ====================
!pip install pandas numpy matplotlib seaborn scikit-learn statsmodels openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
from scipy.stats import ttest_ind
import warnings
warnings.filterwarnings('ignore')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´
plt.style.use('seaborn-v0_8')
sns.set_palette("Set2")
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (12, 8)

# ==================== Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ ====================
from google.colab import files

# Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„
uploaded = files.upload()
filename = list(uploaded.keys())[0]

# Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df = pd.read_excel(filename)
print("Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙØ§ÛŒÙ„:")
print(df.columns.tolist())

# ==================== Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ====================
# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ
if 'Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…' in df.columns:
    df['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…'] = df['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…'].astype(str)
    df['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø¹Ø¯Ø¯'] = df['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…'].apply(lambda x: 1 if x.strip() in ['Ø¨Ù„Ù‡', '1', 'yes', 'true'] else 0)

# Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ±ç¼ºå¤±
df = df.dropna()

print(f"ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ: {len(df)}")

# ==================== Û±. ØªØ­Ù„ÛŒÙ„ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú©ÛŒÙÛŒ ====================
print("="*50)
print("ØªØ­Ù„ÛŒÙ„ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú©ÛŒÙÛŒ")
print("="*50)

categorical_vars = ['Ø¬Ù†Ø³ÛŒØª', 'Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡', 'Ø³Ø§Ø¨Ù‚Ù‡_Ù…Ø´Ø±ÙˆØ·ÛŒ']

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for i, var in enumerate(categorical_vars):
    if var in df.columns:
        counts = df[var].value_counts()
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±
        labels_en = []
        for label in counts.index:
            if 'Ù…Ø±Ø¯' in str(label): labels_en.append('Male')
            elif 'Ø²Ù†' in str(label): labels_en.append('Female')
            elif 'Ø¯Ø§Ø±Ø¯' in str(label): labels_en.append('Yes')
            elif 'Ù†Ø¯Ø§Ø±Ø¯' in str(label): labels_en.append('No')
            else: labels_en.append(f'Cat {i+1}')
        
        axes[i].pie(counts.values, labels=labels_en, autopct='%1.1f%%', startangle=90)
        axes[i].set_title(f'Distribution of {var}')
        
        print(f"\n{var}:")
        print(counts)
        print(f"Ù†Ø³Ø¨Øª: {(counts/len(df)*100).round(1)}%")

plt.tight_layout()
plt.show()

# ØªØ­Ù„ÛŒÙ„ Ø§Ø±ØªØ¨Ø§Ø· Ø¬Ù†Ø³ÛŒØª Ùˆ Ù…Ø´Ø±ÙˆØ·ÛŒ
if 'Ø¬Ù†Ø³ÛŒØª' in df.columns and 'Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…' in df.columns:
    cross_tab = pd.crosstab(df['Ø¬Ù†Ø³ÛŒØª'], df['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…'])
    cross_tab_percent = pd.crosstab(df['Ø¬Ù†Ø³ÛŒØª'], df['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…'], normalize='index') * 100
    
    print("\nÙ†Ø³Ø¨Øª Ù…Ø´Ø±ÙˆØ·ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¬Ù†Ø³ÛŒØª (%):")
    print(cross_tab_percent.round(1))

# ==================== Û². ØªØ­Ù„ÛŒÙ„ ØªÙˆØµÛŒÙÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ ====================
print("\n" + "="*50)
print("Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø±ÙˆØ· Ùˆ ØºÛŒØ±Ù…Ø´Ø±ÙˆØ·")
print("="*50)

if 'Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…' in df.columns:
    grouped = df.groupby('Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…')
    
    print("ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø± Ù‡Ø± Ú¯Ø±ÙˆÙ‡:")
    print(grouped.size())
    
    numeric_vars = ['Ø³Ù†', 'Ù…Ø¹Ø¯Ù„_ØªØ±Ù…_Ù‚Ø¨Ù„(0-20)', 'ØªØ¹Ø¯Ø§Ø¯_ÙˆØ§Ø­Ø¯_Ø§Ø®Ø°Ø´Ø¯Ù‡', 
                   'Ø³Ø§Ø¹Ø§Øª_Ù…Ø·Ø§Ù„Ø¹Ù‡_Ù‡ÙØªÚ¯ÛŒ', 'Ø¯Ø±ØµØ¯_Ø­Ø¶ÙˆØ±_Ø¯Ø±_Ú©Ù„Ø§Ø³', 
                   'Ø³Ø§Ø¹Ø§Øª_Ú©Ø§Ø±_Ù‡ÙØªÚ¯ÛŒ', 'Ø²Ù…Ø§Ù†_Ø±ÙØª_Ùˆ_Ø¢Ù…Ø¯_Ø±ÙˆØ²Ø§Ù†Ù‡(Ø¯Ù‚ÛŒÙ‚Ù‡)',
                   'ÙØ´Ø§Ø±_Ù…Ø§Ù„ÛŒ(1-5)', 'Ú©ÛŒÙÛŒØª_Ø§ÛŒÙ†ØªØ±Ù†Øª(1-5)', 
                   'Ø§Ø¶Ø·Ø±Ø§Ø¨_Ø§Ù…ØªØ­Ø§Ù†(1-5)', 'Ø³Ø§Ø¹Ø§Øª_Ø®ÙˆØ§Ø¨(Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†_Ø´Ø¨Ø§Ù†Ù‡)']
    
    numeric_vars = [var for var in numeric_vars if var in df.columns]
    
    numeric_stats = grouped[numeric_vars].mean()
    print("\nÙ…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø´Ø±ÙˆØ·ÛŒ:")
    print(numeric_stats.round(2))

# ==================== Û³. Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù„Ø¬Ø³ØªÛŒÚ© ====================
print("\n" + "="*50)
print("Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù„Ø¬Ø³ØªÛŒÚ© - Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø´Ø±ÙˆØ·ÛŒ")
print("="*50)

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
X = df.drop(['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø§ÛŒÙ†_ØªØ±Ù…', 'Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø¹Ø¯Ø¯', 'Ø´Ù†Ø§Ø³Ù‡'], axis=1, errors='ignore')
X = pd.get_dummies(X, drop_first=True)
y = df['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø¹Ø¯Ø¯']

X = X.dropna()
y = y.loc[X.index]

# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, 
                                                    random_state=42, 
                                                    stratify=y)

# Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
logreg = LogisticRegression(random_state=42, max_iter=1000)
logreg.fit(X_train_scaled, y_train)

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
y_pred = logreg.predict(X_test_scaled)
y_pred_proba = logreg.predict_proba(X_test_scaled)[:, 1]

print("Ú¯Ø²Ø§Ø±Ø´ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:")
print(classification_report(y_test, y_pred))

print("Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ:")
print(confusion_matrix(y_test, y_pred))

# Ù†Ù…ÙˆØ¯Ø§Ø± ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, 
         label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Ù…Ù†Ø­Ù†ÛŒ ROC - Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù„Ø¬Ø³ØªÛŒÚ©')
plt.legend(loc='lower right')
plt.show()

# ==================== Û´. ØªØ­Ù„ÛŒÙ„ Ø¹ÙˆØ§Ù…Ù„ Ù…Ù‡Ù… ====================
print("\n" + "="*50)
print("Ø¹ÙˆØ§Ù…Ù„ Ù…Ù‡Ù… Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø´Ø±ÙˆØ·ÛŒ")
print("="*50)

feature_importance = pd.DataFrame({
    'feature': X.columns,
    'coefficient': logreg.coef_[0],
    'abs_coefficient': np.abs(logreg.coef_[0])
}).sort_values('abs_coefficient', ascending=False)

print("Ø¶Ø±Ø§ÛŒØ¨ Ù…Ø¯Ù„ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª):")
print(feature_importance[['feature', 'coefficient']].head(10))

# Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
plt.figure(figsize=(12, 8))
top_features = feature_importance.head(10)
colors = ['red' if coef < 0 else 'green' for coef in top_features['coefficient']]

plt.barh(top_features['feature'], top_features['abs_coefficient'], color=colors)
plt.xlabel('Ø§Ù‡Ù…ÛŒØª (Ù…Ù‚Ø¯Ø§Ø± Ù…Ø·Ù„Ù‚ Ø¶Ø±ÛŒØ¨)')
plt.title('Ø¯Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ù‡Ù… Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø´Ø±ÙˆØ·ÛŒ')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# ==================== Ûµ. Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ ====================
print("\n" + "="*50)
print("Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ Ø¨Ø§ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ù†ØªØ®Ø¨")
print("="*50)

selected_features = feature_importance.head(5)['feature'].tolist()

# Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
X_selected = df[selected_features].copy()
X_selected = sm.add_constant(X_selected)
y = df['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø¹Ø¯Ø¯']

# Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„
model = sm.OLS(y, X_selected).fit()
print(model.summary())

# ==================== Û¶. ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ ====================
print("\n" + "="*50)
print("ØªØ­Ù„ÛŒÙ„ Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ")
print("="*50)

numeric_vars = [var for var in df.select_dtypes(include=[np.number]).columns if var != 'Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø¹Ø¯Ø¯']
correlation_matrix = df[numeric_vars].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', 
            center=0, fmt='.2f', square=True)
plt.title('Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ')
plt.tight_layout()
plt.show()

# Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù
target_corr = correlation_matrix['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø¹Ø¯Ø¯'].sort_values(key=abs, ascending=False)
print("Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§ Ù…Ø´Ø±ÙˆØ·ÛŒ:")
print(target_corr.round(3))

# ==================== Û·. Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ====================
print("\n" + "="*60)
print("Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬")
print("="*60)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
total_students = len(df)
probation_students = df['Ù…Ø´Ø±ÙˆØ·ÛŒ_Ø¹Ø¯Ø¯'].sum()
probation_rate = (probation_students / total_students) * 100

print(f"ğŸ“ˆ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ Ù†Ù…ÙˆÙ†Ù‡:")
print(f"â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù†: {total_students:,}")
print(f"â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ù…Ø´Ø±ÙˆØ·: {probation_students:,}")
print(f"â€¢ Ù†Ø±Ø® Ù…Ø´Ø±ÙˆØ·ÛŒ: {probation_rate:.1f}%")

print(f"\nğŸ” ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:")
key_findings = [
    "âœ… Ù…Ø¹Ø¯Ù„ ØªØ±Ù… Ù‚Ø¨Ù„ Ù‚ÙˆÛŒâ€ŒØªØ±ÛŒÙ† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù…Ø´Ø±ÙˆØ·ÛŒ",
    "âœ… Ø³Ø§Ø¹Ø§Øª Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ Ø­Ø¶ÙˆØ± Ø¯Ø± Ú©Ù„Ø§Ø³ ØªØ§Ø«ÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø± Ù…ÙˆÙÙ‚ÛŒØª",
    "âœ… ÙØ´Ø§Ø± Ù…Ø§Ù„ÛŒ Ùˆ Ø³Ø§Ø¹Ø§Øª Ú©Ø§Ø± Ø²ÛŒØ§Ø¯ Ø§Ø² Ø¹ÙˆØ§Ù…Ù„ Ø®Ø·Ø± Ù‡Ø³ØªÙ†Ø¯",
    "âœ… Ø®ÙˆØ§Ø¨ Ù†Ø§Ú©Ø§ÙÛŒ Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªØ­ØµÛŒÙ„ÛŒ ØªØ§Ø«ÛŒØ± Ù…Ù†ÙÛŒ Ø¯Ø§Ø±Ø¯",
    f"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ù‚Øª {logreg.score(X_test_scaled, y_test):.1%} Ù…Ø´Ø±ÙˆØ·ÛŒ Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"
]

for finding in key_findings:
    print(finding)

print(f"\nğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¹Ù…Ù„ÛŒ:")
recommendations = [
    "ğŸ¯ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø­Ù…Ø§ÛŒØª Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø¨Ø§ Ù…Ø¹Ø¯Ù„ Ù¾Ø§ÛŒÛŒÙ†",
    "â° Ú©Ø§Ø±Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø²Ù…Ø§Ù† Ùˆ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡",
    "ğŸ’° Ú©Ù…Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ù‡Ø¯ÙÙ…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ ÙØ´Ø§Ø± Ø§Ù‚ØªØµØ§Ø¯ÛŒ",
    "ğŸ˜´ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡Ø¯Ø§Ø´Øª Ø®ÙˆØ§Ø¨ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³ØªØ±Ø³",
    "ğŸ“Š Ø³ÛŒØ³ØªÙ… Ù‡Ø´Ø¯Ø§Ø± Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø¯Ø± Ø®Ø·Ø±"
]

for i, rec in enumerate(recommendations, 1):
    print(f"{i}. {rec}")

# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
final_report = {
    "ØªØ§Ø±ÛŒØ®_ØªØ­Ù„ÛŒÙ„": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M"),
    "ØªØ¹Ø¯Ø§Ø¯_Ù†Ù…ÙˆÙ†Ù‡": total_students,
    "Ù†Ø±Ø®_Ù…Ø´Ø±ÙˆØ·ÛŒ": f"{probation_rate:.1f}%",
    "Ø¯Ù‚Øª_Ù…Ø¯Ù„": f"{logreg.score(X_test_scaled, y_test):.1%}",
    "Ù‚ÙˆÛŒâ€ŒØªØ±ÛŒÙ†_Ø¹ÙˆØ§Ù…Ù„": feature_importance.head(3)['feature'].tolist(),
    "AUC_ROC": f"{roc_auc:.3f}"
}

import json
with open('Ù†ØªØ§ÛŒØ¬_ØªØ­Ù„ÛŒÙ„_Ù†Ù‡Ø§ÛŒÛŒ.json', 'w', encoding='utf-8') as f:
    json.dump(final_report, f, ensure_ascii=False, indent=2)

print(f"\nâœ… Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ 'Ù†ØªØ§ÛŒØ¬_ØªØ­Ù„ÛŒÙ„_Ù†Ù‡Ø§ÛŒÛŒ.json' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
print("âœ… ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")

# Ù†Ù…Ø§ÛŒØ´ Ù…Ù†Ø§Ø¨Ø¹
print(f"\nğŸ“š Ù…Ù†Ø§Ø¨Ø¹ Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡:")
resources = [
    "1. James, G., et al. (2021). An Introduction to Statistical Learning",
    "2. Field, A., et al. (2012). Discovering Statistics Using R", 
    "3. Ù…Ø¤Ù…Ù†ÛŒØŒ Ù…. (1400). Ø¢Ù…Ø§Ø± Ùˆ Ø§Ø­ØªÙ…Ø§Ù„ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¢Ù†",
    "4. Ù…Ø¤Ù…Ù†ÛŒØŒ Ù…. (1399). ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Python",
    "5. Ù†ÙˆØ±ÙˆØ²ÛŒØŒ Ø¹. (1398). ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ Ø¨Ø§ SPSS"
]

for resource in resources:
    print(resource)